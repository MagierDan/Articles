Get the legacy code tested or die tryin'
===================

Une des constantes de notre travail de développeur est de voir sans cesse grossir nos applications lorsque le métier nous demande de rajouter de nouvelles fonctionnalités. Une autre de ces constantes, lorsque nous sommes consultants et que nous arrivons sur une nouvelle mission, est de voir des applications assez volumineuses. En général ces dernières n'ont pas un niveau de couverture de tests à l'égal de leur "démesure" et nous devons modifier certaines de leurs fonctionnalités. Ces deux constantes (qui ne sont pas les seules) résument assez bien notre lot quotidien. Si dans le premier cas, il nous est assez facile de commencer le développement en partant sur des bonnes pratiques telle que le TDD, dans le second cas sans une bonne couverture de tests, la chose se corse un peu.

Il existe pourtant des techniques qui permettent de définir un filet de sécurité lors d'une modification ou d'un refactoring.

Golden Master (GM)
-------------
Le but est de générer ou de sélectionner un vaste jeu de donnés à fournir en entrée d'une méthode d'une version de l'application que vous savez correcte. Il faudra ensuite enregistrer les résultats de l'exécution de chacun des éléments de ce jeu. Par la suite à chaque modification de code, vous exécutez le jeu de données d'entrée. Lors d'une évolution, si différence il y a entre le résultat de l'ancienne et de la nouvelle version, il faudra vérifier si celle-ci correspond ou non à ce qui est nouvellement attendu (ce qui définira un nouveau GM), sinon corriger le bug dans le code.  Dans le cas d'un refactoring les résultats doivent être identiques, donc si différence il y a, il faudra corriger cela. En fonction de la taille du jeu de données, la construction de celui-ci tout comme sa maintenance peut s'avérer ardue. Parfois ce sera même du one shot. La réalisation de ce type de jeu de tests peu s'avérer coûteuse. Ce coût est toutefois moindre en principe que si l'on cherchait a réaliser du Test Unitaire (TU) sur le bout de code à modifier ou si l'on tente de corriger les bugs dûs au modification suite à une mise en production.

Record and Play (RaP)
-------------
Autre technique: le Record and Play (RaP). Le but étant ici d'enregistrer un enchaînement d'actions sur une IHM. Comme pour le golden master, la réalisation des enregistrements dépendra du nombre d'actions et de leur combinaisons possibles. Leur maintenance s'avère par ailleurs plus pénible que celle du GM. Sélénium est une librairie assez connue le permettant pour des applications web. Il existe aussi d'autres outils payant qui permettent aussi de tester des applications desktop ou mobile, par exemple Ranorex. Il est possible d'utiliser Sélénium autrement qu'en RaP et d'écrire du tests plus facilement maintenable. Je conseillerais dans ce cas la librairie Simplelenium.

Technique de Parallel Runs
-------------
D'autres techniques existent. J'ai lu il y a peu cet article. Il présentait un framework Ruby "Scientist" de Github utilisé dans le cadre de refactoring de code (Par ailleurs ce dernier a semble-t-il déjà un pendant java: "Experiment4j"). Pour être plus précis, Github avait besoin de refactorer sa fonction de merge. Comme vous pouvez vous en doutez, cette fonction est assez sensible et le moindre problème peu avoir de sérieuses répercussions pour leurs utilisateurs. De plus le nombre de cas de merge possible était conséquent. Leur framework leur a permis de comparer le résultat de l'exécution en production de leur ancienne méthode de merge avec la nouvelle en appelant les deux successivement dans le code. En comparant les différentes et nombreuses exécutions du legacy et de leur nouveau code, les développeurs pouvaient combler les différences constatées. Après corrections, est relivrée en production une version encore plus proche en terme de résultat que l'ancienne. Après plusieurs itérations et une fois le nombre de différences entre les deux fonctions de merge devenu nul, il ont supprimé l'appel de l'ancien code et rendu opérationnel le nouveau. Lors de la phase de comparaison, la vérité est et reste le code legacy. C'est donc son exécution qui est prise en compte pour tout ce qui est traitement fait par l'utilisateur.

On peut trouver ou non audacieuse l'idée d'insérer dans du code à mettre en production une double exécution (même si très intrusive comme méthode).

Il est même possible de voir déjà d'autres éléments à rajouter à ce framework. Et si par exemple l'ensemble des tests nous manquant provenait de toutes les exécutions faites sur la production par les utilisateurs sur le code à refactorer. Si nous n'avions qu'à enregistrer les paramètres d'entrée de nos méthodes ainsi que le résultat, de les filtrer pour ensuite générer des classes de tests unitaires afin d'augmenter notre couverture de tests. Ces derniers ne coûteraient pas très cher en principe (si ce n'est la mise en place des outils adéquats pour les générer) et permettrait de mettre en place un filet de sécurité pour pouvoir refactorer le legacy. Voici comment pourrait naître un nouveau concept, celui de Production Based Testing (PBT) (à ne pas confondre avec le Property Based Testing)

Production Based Testing
-------------
Ce concept est assez proche de la création d'un GM. A ceci prêt que le jeu de données est générée par les utilisateurs en production. Par contre, sa taille peut être dès lors très grande comme pour le GM (tout dépend de la volumétrie d'exécution du code à modifier). Il faudra donc en diminuer la taille par tout moyen (de préférence de façon automatisée ou semi-automatisée) afin d'avoir l'ensemble de tests minimum qui permet d'avoir le bon filet de sécurité. L'ensemble de ces tests pourra être rejoué sur un environnement rafraîchi à partir de la production.

Au delà de leur coût assez faible, ces tests auraient le mérite de correspondre à ce qui est attendu par le métier. En règle général rares sont les documentations fonctionnelles qui soient à jour. De plus, intéresser les fonctionnels sur ce qui a déjà été produit n'est pas forcément chose aisée surtout quand aucune plus-value n'est rajoutée; et quand le métier est riche et complexe, cela est encore plus dur. Par contre, si ce qui tourne en production fonctionne "bien" (c'est à dire correspond parfaitement à ce qui est attendu du métier) alors cette méthodologie aurait le mérite de fournir une couverture de tests convenable. Il faudra par contre que l'ensemble des cas soit joué en production au risque d'avoir des trous dans les jeux de tests générés.

Maintenant et comme dit plus haut, si l'approche des frameworks (Scientist et Experiment4J) est ingénieuse, ils ont aussi un gros défaut à savoir celui d'être lourd. C'est en discutant de ces frameworks et cette idée de TU générés par la production avec un collègue, David CARAMELO, que ce dernier me dit, avec de lueurs étoilées dans le regard, qu'il pensait être possible de supprimer l'ajout du code refactoré en production en mettant en place un agent java. Au lieu d'avoir le code legacy et refactoré en production (et donc la double exécution), il suffirait d'espionner via l'agent les méthodes dont on souhaite générer les TU. En même temps, dire cela est facile, mais la conception de cet outil peut être un peu plus complexe que cela. De plus, il faut aussi voir que certaines exécutions de code peuvent être plus complexes qu'il n'y parait, et peut être ne pas correspondre à ce concept. Pour l'instant ce n'est qu'un concept, et en parler est facile. Dans la vraie vie malheureusement, on se heurte parfois (souvent) à des cas bien particuliers qui ne nous permettent pas d'utiliser des solutions "standards".

Comme dit plus haut la volumétrie des tests générés peut être impactante sur la maintenance de ceux-ci. Il faudra aussi pouvoir nommer convenablement les TU générés et cela se fera certainement avec une intervention humaine. Lorsque l'on parle d'intervention humaine ici, ce serait de préférence de manière semi-automatique (une interface web listant les entrées et sorties des exécutions et nous permettant de trier en fonction de critères). Donc ces tests auront un coût un peu plus fort que prévu. Moins qu'un GM construit à la main, peut être pas, mais au moins ces tests ne seront pas du one-shot (si évolution fonctionnelle il y a par la suite) et augmenteront la couverture de tests relative à la fonctionnalité refactorée.

Petit point que certains auront remarqué, il serait par contre tout à fait possible d'utiliser un outil de ce type pour construire un GM de manière complètement automatisée.

L'intérêt d'un tel outil est facilement perceptible, notamment pour ceux qui travaillent sur un code peu documenté fonctionnellement parlant mais qui possède le comportement attendu par le métier. Il est tout à fait possible que prochainement un framework de ce type puisse arriver pour nous simplifier la vie. En tout cas, le concept est maintenant présent et avec lui peut être un peu l'espoir qu'il permettra d'améliorer petit à petit le code legacy rencontré.

Quelle technique choisir
-------------
Lorsque votre legacy est une boîte noire et que vous ne savez plus quoi faire pour créer ce  filet, les techniques suivantes peuvent vous aider grandement: GM, du Rap ou bien du PBT


Conclusion
-------------
Entre le GM, le RaP, Experiment4J et peut être un jour un outil de TBP, il existe des moyens de se lancer dans un refactoring avec la possibilité de créer un filet de sécurité. La solution à sélectionner dépendra de plusieurs critères (temps, ressources, etc...), en espérant que celle choisit sera la meilleure en fonction du contexte rencontré.

